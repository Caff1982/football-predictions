{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "DATA_PATH = 'data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(os.path.join(DATA_PATH, 'season1819.csv'))\n",
    "df2 = pd.read_csv(os.path.join(DATA_PATH, 'season1718.csv'))\n",
    "df3 = pd.read_csv(os.path.join(DATA_PATH, 'season1617.csv'))\n",
    "df4 = pd.read_csv(os.path.join(DATA_PATH, 'season1516.csv'))\n",
    "df5 = pd.read_csv(os.path.join(DATA_PATH, 'season1415.csv'))\n",
    "df6 = pd.read_csv(os.path.join(DATA_PATH, 'season1314.csv'))\n",
    "df7 = pd.read_csv(os.path.join(DATA_PATH, 'season1213.csv'))\n",
    "df8 = pd.read_csv(os.path.join(DATA_PATH, 'season1112.csv'))\n",
    "df9 = pd.read_csv(os.path.join(DATA_PATH, 'season1011.csv'))\n",
    "df10 = pd.read_csv(os.path.join(DATA_PATH, 'season0910.csv'))\n",
    "df11 = pd.read_csv(os.path.join(DATA_PATH, 'season0809.csv'))\n",
    "df12 = pd.read_csv(os.path.join(DATA_PATH, 'season0708.csv'))\n",
    "\n",
    "all_seasons = [df1, df2, df3, df4, df5, df6,\n",
    "               df7, df8, df9, df10,df11,df12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H    0.462061\n",
      "A    0.288596\n",
      "D    0.249342\n",
      "Name: FTR, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephen/Projects/betting_model/env/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5, df6,\n",
    "               df7, df8, df9, df10,df11,df12], ignore_index=True)\n",
    "results = df['FTR'].value_counts(normalize=True)\n",
    "print(results)\n",
    "results.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H    0.476316\n",
      "A    0.336842\n",
      "D    0.186842\n",
      "Name: FTR, dtype: float64\n",
      "H    0.455263\n",
      "A    0.284211\n",
      "D    0.260526\n",
      "Name: FTR, dtype: float64\n",
      "H    0.492105\n",
      "A    0.286842\n",
      "D    0.221053\n",
      "Name: FTR, dtype: float64\n",
      "H    0.413158\n",
      "A    0.305263\n",
      "D    0.281579\n",
      "Name: FTR, dtype: float64\n",
      "H    0.452632\n",
      "A    0.302632\n",
      "D    0.244737\n",
      "Name: FTR, dtype: float64\n",
      "H    0.471053\n",
      "A    0.323684\n",
      "D    0.205263\n",
      "Name: FTR, dtype: float64\n",
      "H    0.436842\n",
      "D    0.284211\n",
      "A    0.278947\n",
      "Name: FTR, dtype: float64\n",
      "H    0.450000\n",
      "A    0.305263\n",
      "D    0.244737\n",
      "Name: FTR, dtype: float64\n",
      "H    0.471053\n",
      "D    0.292105\n",
      "A    0.236842\n",
      "Name: FTR, dtype: float64\n",
      "H    0.507895\n",
      "D    0.252632\n",
      "A    0.239474\n",
      "Name: FTR, dtype: float64\n",
      "H    0.455263\n",
      "A    0.289474\n",
      "D    0.255263\n",
      "Name: FTR, dtype: float64\n",
      "H    0.463158\n",
      "A    0.273684\n",
      "D    0.263158\n",
      "Name: FTR, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for season in all_seasons:\n",
    "    results = season['FTR'].value_counts(normalize=True)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that home advantage has a big effect on outcome. If our classifier was to predict only home wins it would get around 46% correct. We can use this as a benchmark. Let's see how accurate the bookies are now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "home = 0\n",
    "for i in range(len(df)):\n",
    "    favourite = np.argmin([df['B365H'][i], df['B365D'][i], df['B365A'][i]])\n",
    "    if favourite == 0:\n",
    "        home +=1\n",
    "    if df['FTR'][i] == 'H' and favourite == 0:\n",
    "        correct +=1\n",
    "    elif df['FTR'][i] == 'D' and favourite == 1:\n",
    "        correct +=1\n",
    "    elif df['FTR'][i] == 'A' and favourite == 2:\n",
    "        correct +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.549440912080684"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the bookies have pretty good accuracy, around 55%. They seem to rely on home advantage to make their prediction. Also the bookies never predicted any draws in this dataset. \n",
    "\n",
    "Let's split the data to X and y and have a look at the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4560, 40)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/training_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 4560\n",
      "Number of features: 16\n"
     ]
    }
   ],
   "source": [
    "X_all = df.drop(['FTR', 'Unnamed: 0', 'Date', 'HomeTeam', 'AwayTeam', \n",
    "                 'HTGS', 'ATGS', 'HTGC', 'ATGC', 'HM4', 'HM5', 'AM4', \n",
    "                 'AM5', 'HTSF', 'HTSA', 'ATSF', 'ATSA', 'MW', 'HTFormPts', 'ATFormPts',\n",
    "                        'HTFormPtsStr', 'ATFormPtsStr', 'HTSD', 'ATSD'], 1)\n",
    "Y_all = df['FTR']\n",
    "\n",
    "print('Number of matches:', X_all.shape[0])\n",
    "print('Number of features:', X_all.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "We will now split the data into training and testing and scale our data.\n",
    "\n",
    "We can come back and try feature engineering later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_results(results):\n",
    "    transformed = []\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == 'H':\n",
    "            transformed.append(0)\n",
    "        elif results[i] == 'A':\n",
    "            transformed.append(2)\n",
    "        else:\n",
    "            transformed.append(1)\n",
    "    return np.array(transformed)\n",
    "            \n",
    "Y_all = transform_results(Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want continous vars that are integers for our input data, so lets remove any categorical vars\n",
    "def preprocess_features(X):\n",
    "    ''' Preprocesses the football data and converts catagorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)\n",
    "                    \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sca;e our numeric columns\n",
    "scaler = StandardScaler()\n",
    "cols = [['HTP', 'ATP', 'HTGD', 'ATGD', 'DiffFormPts',\n",
    "        'B365H', 'B365D', 'B365A', 'DiffLP', 'DiffPts']]\n",
    "for col in cols:\n",
    "        X_all[col] = scaler.fit_transform(X_all[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HTP', 'ATP', 'B365H', 'B365D', 'B365A', 'HM1_D', 'HM1_L', 'HM1_M',\n",
       "       'HM1_W', 'HM2_D', 'HM2_L', 'HM2_M', 'HM2_W', 'HM3_D', 'HM3_L', 'HM3_M',\n",
       "       'HM3_W', 'AM1_D', 'AM1_L', 'AM1_M', 'AM1_W', 'AM2_D', 'AM2_L', 'AM2_M',\n",
       "       'AM2_W', 'AM3_D', 'AM3_L', 'AM3_M', 'AM3_W', 'HTGD', 'ATGD', 'DiffPts',\n",
       "       'DiffFormPts', 'DiffLP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now we will use the last season for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_all[380:]\n",
    "Y = Y_all[380:]\n",
    "X_test = X_all[:380]\n",
    "y_test = Y_all[:380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1500, max_depth=8,\n",
    "                               min_samples_leaf=3, n_jobs=-1,\n",
    "                               random_state=42, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5947368421052631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5447368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6105263157894737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4789473684210526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5289473684210526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6052631578947368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5342105263157895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5394736842105263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5078947368421053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5710526315789474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5736842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 1500 out of 1500 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "season_len = 380\n",
    "results = []\n",
    "for i in range(12):\n",
    "    idx = i * season_len\n",
    "    X = np.concatenate((X_all[:idx], X_all[idx+380:]))\n",
    "    Y = np.concatenate((Y_all[:idx], Y_all[idx+380:]))\n",
    "    X_test = X_all[idx:idx+380]\n",
    "    y_test = Y_all[idx:idx+380]\n",
    "    \n",
    "    model.fit(X, Y)\n",
    "    y_preds = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_preds)\n",
    "    print(accuracy)\n",
    "    results.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy :0.5517543859649122\n",
      "Min accuracy :0.4789473684210526\n",
      "Max accuracy :0.6105263157894737\n",
      "Standard Deviation :0.03838157869677049\n"
     ]
    }
   ],
   "source": [
    "print(f'Average accuracy :{sum(results)/len(results)}')\n",
    "print(f'Min accuracy :{min(results)}')\n",
    "print(f'Max accuracy :{max(results)}')\n",
    "print(f'Standard Deviation :{np.std(results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = [i for i in range(len(results))]\n",
    "plt.bar(bins, results, align='edge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6052631578947368,\n",
       " 0.5473684210526316,\n",
       " 0.6026315789473684,\n",
       " 0.48157894736842105,\n",
       " 0.5289473684210526,\n",
       " 0.6026315789473684,\n",
       " 0.531578947368421,\n",
       " 0.5394736842105263,\n",
       " 0.5131578947368421,\n",
       " 0.5736842105263158,\n",
       " 0.5263157894736842,\n",
       " 0.5894736842105263]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTP 0.034115580365994366\n",
      "ATP 0.030986248460692277\n",
      "B365H 0.5204356779119939\n",
      "B365D 0.07280107880872572\n",
      "B365A 0.1425309670557084\n",
      "HM1_D 0.008406150418563282\n",
      "HM1_L 0.002973901575991384\n",
      "HM1_M 0.0\n",
      "HM1_W 0.009922785805635173\n",
      "HM2_D 0.0\n",
      "HM2_L 0.0007398032353477757\n",
      "HM2_M 0.0\n",
      "HM2_W 0.0\n",
      "HM3_D 0.0010000226973583227\n",
      "HM3_L 0.001277884712172643\n",
      "HM3_M 0.0\n",
      "HM3_W 0.0\n",
      "AM1_D 0.006225240354927263\n",
      "AM1_L 0.027269319685627576\n",
      "AM1_M 0.0\n",
      "AM1_W 0.011333630059719945\n",
      "AM2_D 0.0\n",
      "AM2_L 0.0\n",
      "AM2_M 0.0\n",
      "AM2_W 0.0017416292807036597\n",
      "AM3_D 0.0005170789866313664\n",
      "AM3_L 0.0024388306914882455\n",
      "AM3_M 0.0\n",
      "AM3_W 0.00043137567442527665\n",
      "HTGD 0.026272414907787967\n",
      "ATGD 0.033394894924648116\n",
      "DiffPts 0.021460292573301877\n",
      "DiffFormPts 0.0199669624497407\n",
      "DiffLP 0.02375822936281478\n"
     ]
    }
   ],
   "source": [
    "for col, importance in zip(X_all.columns, model.feature_importances_):\n",
    "    print(col, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=75, max_depth=2,\n",
    "                                      verbose=1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52.6% is a decent benchmark, this is significantly higher than if the model just predicted home wins but it is still some way off the bookies model. We can now try some other models and look at tuning hyperparameters, we may also want to try some more feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4360.0427            0.82s\n",
      "         2        4292.1078            0.79s\n",
      "         3        4237.4933            0.77s\n",
      "         4        4191.8445            0.75s\n",
      "         5        4155.0363            0.74s\n",
      "         6        4124.0779            0.72s\n",
      "         7        4098.0524            0.71s\n",
      "         8        4076.3416            0.69s\n",
      "         9        4056.9230            0.67s\n",
      "        10        4040.6987            0.66s\n",
      "        20        3951.8395            0.54s\n",
      "        30        3910.3908            0.44s\n",
      "        40        3877.5601            0.33s\n",
      "        50        3854.5687            0.23s\n",
      "        60        3834.3716            0.14s\n",
      "        70        3813.4893            0.05s\n",
      "0.5868421052631579\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4349.9607            0.73s\n",
      "         2        4281.1023            0.71s\n",
      "         3        4224.9148            0.71s\n",
      "         4        4178.7671            0.70s\n",
      "         5        4140.6641            0.69s\n",
      "         6        4109.2384            0.68s\n",
      "         7        4083.0130            0.68s\n",
      "         8        4060.3621            0.68s\n",
      "         9        4041.3657            0.67s\n",
      "        10        4024.7490            0.66s\n",
      "        20        3934.5837            0.53s\n",
      "        30        3890.6726            0.42s\n",
      "        40        3858.3804            0.32s\n",
      "        50        3832.9679            0.23s\n",
      "        60        3809.8004            0.14s\n",
      "        70        3789.7549            0.04s\n",
      "0.5368421052631579\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4359.6475            0.72s\n",
      "         2        4291.5257            0.72s\n",
      "         3        4236.2664            0.70s\n",
      "         4        4190.6665            0.70s\n",
      "         5        4152.6996            0.69s\n",
      "         6        4121.7860            0.67s\n",
      "         7        4095.9439            0.66s\n",
      "         8        4073.2909            0.67s\n",
      "         9        4054.3491            0.67s\n",
      "        10        4037.4719            0.66s\n",
      "        20        3947.8357            0.53s\n",
      "        30        3902.4198            0.42s\n",
      "        40        3872.8814            0.32s\n",
      "        50        3845.7356            0.22s\n",
      "        60        3823.1361            0.13s\n",
      "        70        3800.6266            0.04s\n",
      "0.6078947368421053\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4335.7825            0.73s\n",
      "         2        4262.8956            0.71s\n",
      "         3        4203.4030            0.70s\n",
      "         4        4153.5909            0.70s\n",
      "         5        4113.1245            0.69s\n",
      "         6        4079.3132            0.68s\n",
      "         7        4051.7915            0.68s\n",
      "         8        4027.6603            0.68s\n",
      "         9        4008.2618            0.66s\n",
      "        10        3991.1755            0.65s\n",
      "        20        3898.9305            0.53s\n",
      "        30        3855.7910            0.42s\n",
      "        40        3825.2389            0.32s\n",
      "        50        3799.5548            0.22s\n",
      "        60        3777.3599            0.13s\n",
      "        70        3759.5610            0.04s\n",
      "0.4868421052631579\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4348.8239            0.74s\n",
      "         2        4278.5509            0.73s\n",
      "         3        4221.4572            0.72s\n",
      "         4        4174.8705            0.71s\n",
      "         5        4136.2689            0.69s\n",
      "         6        4104.1201            0.68s\n",
      "         7        4077.5058            0.67s\n",
      "         8        4054.5001            0.66s\n",
      "         9        4035.3728            0.66s\n",
      "        10        4018.3553            0.65s\n",
      "        20        3924.8660            0.53s\n",
      "        30        3878.8044            0.42s\n",
      "        40        3849.5867            0.32s\n",
      "        50        3824.1140            0.23s\n",
      "        60        3803.1576            0.14s\n",
      "        70        3782.2090            0.05s\n",
      "0.55\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4355.9650            0.73s\n",
      "         2        4286.8482            0.72s\n",
      "         3        4231.0745            0.71s\n",
      "         4        4184.4531            0.71s\n",
      "         5        4146.8856            0.70s\n",
      "         6        4115.0286            0.69s\n",
      "         7        4088.4711            0.68s\n",
      "         8        4065.8265            0.66s\n",
      "         9        4046.2026            0.65s\n",
      "        10        4029.6293            0.64s\n",
      "        20        3938.6207            0.53s\n",
      "        30        3895.1643            0.41s\n",
      "        40        3864.2536            0.32s\n",
      "        50        3839.2751            0.22s\n",
      "        60        3818.4440            0.13s\n",
      "        70        3798.1838            0.04s\n",
      "0.5894736842105263\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4345.4587            0.72s\n",
      "         2        4276.7064            0.71s\n",
      "         3        4220.4969            0.71s\n",
      "         4        4174.2681            0.70s\n",
      "         5        4136.7813            0.69s\n",
      "         6        4104.7115            0.68s\n",
      "         7        4077.8119            0.68s\n",
      "         8        4055.3949            0.67s\n",
      "         9        4035.7441            0.65s\n",
      "        10        4019.0767            0.64s\n",
      "        20        3929.7362            0.53s\n",
      "        30        3884.4254            0.42s\n",
      "        40        3854.9364            0.32s\n",
      "        50        3829.1788            0.22s\n",
      "        60        3807.4018            0.13s\n",
      "        70        3789.0337            0.04s\n",
      "0.531578947368421\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4347.2933            0.74s\n",
      "         2        4276.4649            0.72s\n",
      "         3        4219.2243            0.71s\n",
      "         4        4170.6168            0.70s\n",
      "         5        4131.6113            0.69s\n",
      "         6        4099.0842            0.69s\n",
      "         7        4072.2045            0.67s\n",
      "         8        4049.5979            0.67s\n",
      "         9        4030.3586            0.65s\n",
      "        10        4013.9460            0.64s\n",
      "        20        3923.1472            0.53s\n",
      "        30        3879.4612            0.42s\n",
      "        40        3847.8335            0.32s\n",
      "        50        3823.1539            0.23s\n",
      "        60        3801.3713            0.13s\n",
      "        70        3782.6994            0.05s\n",
      "0.5473684210526316\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4345.3892            0.83s\n",
      "         2        4271.9512            0.82s\n",
      "         3        4212.1768            0.82s\n",
      "         4        4163.7250            0.80s\n",
      "         5        4123.5765            0.80s\n",
      "         6        4090.4305            0.80s\n",
      "         7        4062.7770            0.79s\n",
      "         8        4039.7529            0.78s\n",
      "         9        4019.4148            0.76s\n",
      "        10        4001.6460            0.75s\n",
      "        20        3910.2769            0.61s\n",
      "        30        3863.8360            0.49s\n",
      "        40        3834.9322            0.37s\n",
      "        50        3811.2822            0.26s\n",
      "        60        3788.5445            0.15s\n",
      "        70        3766.7283            0.05s\n",
      "0.49473684210526314\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4359.0634            0.73s\n",
      "         2        4289.7982            0.72s\n",
      "         3        4233.3138            0.71s\n",
      "         4        4186.8304            0.70s\n",
      "         5        4148.4223            0.69s\n",
      "         6        4117.4177            0.68s\n",
      "         7        4091.3249            0.67s\n",
      "         8        4069.2442            0.66s\n",
      "         9        4050.4782            0.66s\n",
      "        10        4034.4783            0.66s\n",
      "        20        3947.9269            0.55s\n",
      "        30        3906.0332            0.46s\n",
      "        40        3876.5970            0.36s\n",
      "        50        3851.0337            0.26s\n",
      "        60        3830.6659            0.15s\n",
      "        70        3808.9262            0.05s\n",
      "0.5789473684210527\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4348.8008            0.74s\n",
      "         2        4278.7429            0.72s\n",
      "         3        4220.8815            0.72s\n",
      "         4        4173.6451            0.71s\n",
      "         5        4134.6771            0.82s\n",
      "         6        4102.3274            0.79s\n",
      "         7        4074.6902            0.76s\n",
      "         8        4051.7236            0.74s\n",
      "         9        4032.4780            0.76s\n",
      "        10        4015.6767            0.73s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        20        3923.6665            0.57s\n",
      "        30        3878.1876            0.44s\n",
      "        40        3847.3428            0.34s\n",
      "        50        3821.1480            0.24s\n",
      "        60        3800.3173            0.14s\n",
      "        70        3779.5801            0.05s\n",
      "0.5210526315789473\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        4352.4687            0.74s\n",
      "         2        4284.1838            0.73s\n",
      "         3        4229.1495            0.72s\n",
      "         4        4183.4466            0.72s\n",
      "         5        4146.6318            0.71s\n",
      "         6        4115.7122            0.70s\n",
      "         7        4090.2840            0.69s\n",
      "         8        4069.1244            0.67s\n",
      "         9        4050.6658            0.66s\n",
      "        10        4034.4089            0.64s\n",
      "        20        3950.2475            0.53s\n",
      "        30        3907.7847            0.42s\n",
      "        40        3879.4658            0.32s\n",
      "        50        3853.6639            0.23s\n",
      "        60        3832.2739            0.13s\n",
      "        70        3813.1632            0.04s\n",
      "0.6052631578947368\n"
     ]
    }
   ],
   "source": [
    "season_len = 380\n",
    "results = []\n",
    "for i in range(12):\n",
    "    idx = i * season_len\n",
    "    X = np.concatenate((X_all[:idx], X_all[idx+380:]))\n",
    "    Y = np.concatenate((Y_all[:idx], Y_all[idx+380:]))\n",
    "    X_test = X_all[idx:idx+380]\n",
    "    y_test = Y_all[idx:idx+380]\n",
    "    \n",
    "    model = gb_model\n",
    "    model.fit(X, Y)\n",
    "    y_preds = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_preds)\n",
    "    print(accuracy)\n",
    "    results.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy :0.5530701754385966\n",
      "Min accuracy :0.4868421052631579\n",
      "Max accuracy :0.6078947368421053\n",
      "Standard Deviation :0.039207190093164535\n"
     ]
    }
   ],
   "source": [
    "print(f'Average accuracy :{sum(results)/len(results)}')\n",
    "print(f'Min accuracy :{min(results)}')\n",
    "print(f'Max accuracy :{max(results)}')\n",
    "print(f'Standard Deviation :{np.std(results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
